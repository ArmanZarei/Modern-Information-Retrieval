{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-b2UsRU4P9JF"
   },
   "source": [
    "<div style=\"direction:rtl;line-height:300%;\">\n",
    "<font face=\"XB Zar\" size=5>\n",
    "<div align=center>\n",
    "<font face=\"B Titr\" size=5>\n",
    "<p></p><p></p>\n",
    "بسمه تعالی\n",
    "<p></p>\n",
    "</font>\n",
    "<p></p>\n",
    "<font>\n",
    "<br>\n",
    "درس بازیابی پیشرفته اطلاعات\n",
    "<br>\n",
    "مدرس: دکتر سلیمانی\n",
    "</font>\n",
    "<p></p>\n",
    "<br>\n",
    "<font>\n",
    "<b>فاز اول پروژه</b>\n",
    "</font>\n",
    "<br>\n",
    "<br>\n",
    "موعد تحویل: ۲۸ اسفند ۱۳۹۹\n",
    "<br>\n",
    "<br>\n",
    "<font size=4.8>\n",
    "دستیاران آموزشی: نیما جمالی، آرمین سعادت، ایمان غلامی\n",
    "</font>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<font>\n",
    "دانشگاه صنعتی شریف\n",
    "<br>\n",
    "دانشکده مهندسی کامپیوتر\n",
    "<br>\n",
    "<br>\n",
    "</font>\n",
    "</div>\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y3YoNVsQR3zO"
   },
   "source": [
    "<p></p>\n",
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=4>\n",
    "    <h1>\n",
    "    <b>مقدمه</b>\n",
    "    </h1>\n",
    "    <p></p>\n",
    "    <p></p>\n",
    "    هدف از فاز اول پروژه، طراحی و پیاده‌سازی سیستم بازیابی اطلاعات برای مجموعه دادگان تعیین شده می‌باشد.<br>\n",
    "    اولین مرحله، پیش‌پردازش مجموعه دادگان است. پس از آن نمایه‌ها با ویژگی‌های خواسته شده پیاده‌سازی می‌شود. در گام بعدی به ذخیره و بازخوانی نمایه‌ها به همراه روش‌های فشرده‌سازی پرداخته می‌شود. پس از آن تکنیک‌های اصلاح پرسمان پیاده‌سازی شده و در نهایت جستجو روی دادگان صورت می‌گیرد. همچنین در بخش آخر با پیاده‌سازی برخی معیارهای ارزیابی، عملکرد سیستم مورد سنجش قرار می‌گیرد.<br><br>\n",
    "     توضیحات مربوط به هر بخش در ادامه آمده است که اهداف، محدودیت‌ها و خواسته‌های آن بخش را مشخص می‌کند.\n",
    "     در هر بخش توابعی مشخص شده است که محتوای آن با کد نوشته شده توسط شما باید پر شود. شما می‌توانید در همین فایل، سیستم بازیابی خود را پیاده‌سازی کنید یا فایل‌های خودتان را ایمپورت کرده و از آن‌ها استفاده کنید. در هر صورت تمامی کدهای لازم را به همراه این نوت‌بوک ارسال کنید. همچنین در نظر داشته باشید که ملاک اصلی نمره‌دهی شما اجرای صحیح توابع مشخص‌شده در این نوت‌بوک می‌باشد. بنابراین از صحت اجرای نوت‌بوک خود اطمینان پیدا کنید.<br><br>\n",
    "     تنها زبان قابل قبول برای پروژه پایتون است. محدودیت استفاده از کتاب‌خانه‌های آماده در هر بخش مشخص شده است. پروژه ۱۱۵ نمره دارد که ۱۵ نمره از آن امتیازی و مربوط به بخش اصلاح پرسمان می‌باشد.\n",
    "    \n",
    "    \n",
    "</font>\n",
    "</div>\n",
    "<p></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jaR5CS_khMQB"
   },
   "source": [
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=4>\n",
    "    <h1>\n",
    "    <b>مجموعه دادگان</b>\n",
    "    </h1>\n",
    "    مجموعه دادگان مورد بررسی در این پروژه از سایت kaggle فراهم شده است. این مجموعه شامل اطلاعات ۶۰۰۰ فیلم سینمایی از سال ۱۹۰۴ تا ۲۰۱۷ است. داده‌ها در قالب فایل csv   دارای ستون‌‌های plot، title، id می‌باشد. id یک شناسه یکتا برای هر فیلم است که برای ارزیابی بهتر عملکرد شما به داده‌ها اضافه شده است و در مجموعه دادگان اصلی وجود نداشته است. همانطور که می‌دانید برای پیاده‌سازی نمایه باید به هر داکیومنت یک شناسه اختصاص بدهید. <b>شناسه مربوط به هر داکیومنت باید id ذکر شده برای آن در مجموعه دادگان باشد.</b> هر فیلم از دو بخش title و plot  تشکیل شده است که از این دو بخش در ساخت نمایه و جستجو استفاده می‌شود. plot خلاصه‌ای از طرح داستان فیلم است.<br>\n",
    "    علاوه بر مجموعه دادگان اصلی، تعدای پرسمان در اختیار شما قرار گرفته است. همچنین جواب مطلوب هر یک از این پرسمان‌ها نیز فراهم شده که طبیعتا زیرمجموعه‌ای از مجموعه دادگان است. شما باید از این پرسمان‌ها و نتیجه مورد انتظار هر کدام برای ارزیابی سامانه خود استفاده کنید.\n",
    "    پرسمان‌ها و شناسه فیلم‌های بازیابی شده مورد انتظار از هر پرسمان در فایل validation.json آمده است. توضیحات بیشتر در رابطه با استفاده از این فایل در بخش ارزیابی آمده است.\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PxNntHofmHHH"
   },
   "source": [
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=4>\n",
    "    <h1>\n",
    "    <b>پیش‌پردازش و آماده‌سازی داده‌ها (۱۵ نمره)</b>\n",
    "    </h1>\n",
    "    در این بخش ابتدا داده‌ها را از فایل بخوانید. برای آماده‌سازی متن می‌توانید از کتاب‌خانه‌های آماده استفاده کنید. یکی از کتاب‌خانه‌های معروف برای این کار <a href=\"https://www.nltk.org/\">NLTK</a> است اما در انتخاب روش پیاده‌سازی این بخش مختارید. برای این بخش باید تابع ()prepare_text را تکمیل کنید. این تابع یک متن انگلیسی ورودی گرفته و توکن‌‌های مربوط به آن‌را در قالب یک لیست خروجی می‌دهد. متن ورودی در عمل تایتل یا طرح داستان هر فیلم است. دقت کنید که لیست خروجی شامل تعدادی توکن است که عملیات case folding، stemming و lemmatization روی آن‌ها اجرا شده است. در ضمن علائم نگارشی نباید به عنوان توکن در نظر گرفته شود. در کد زیر یک نمونه ورودی و خروجی نمایش داده شده است. با توجه به نحوه پیاده‌سازی انواع بازگردانی به ریشه قابل قبول است.\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "<b>گزارش : </b>\n",
    "    در این قسمت صرفا package های استفاده شده را import میکنیم\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Arman_PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Arman_PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "import string\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import json\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "<b>گزارش : </b>\n",
    "   برای پیاده سازی این قسمت از nltk استفاده کردی و عملیت های گفته شده را به ترتیب انجام می دهیم . \n",
    "    هر قسمت کامنت گزاری شده ولی برای افزایش سرعت همه این مراحل در یک مرحله (One Shot) انجام شده اند که در انتها مشاهده می کنید\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9aa16Puk-if0",
    "outputId": "2e6a2c95-41d5-4146-a27b-5ceba034e291"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['edvard',\n",
       " 'be',\n",
       " 'a',\n",
       " 'runner',\n",
       " 'he',\n",
       " 'be',\n",
       " 'always',\n",
       " 'run',\n",
       " 'the',\n",
       " 'adventure',\n",
       " 'of',\n",
       " 'sherlock',\n",
       " 'holmes',\n",
       " 'students',\n",
       " 'class']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def prepare_text(raw_text):\n",
    "    # ----------- Remove all punctuations ----------- #\n",
    "    trantab = str.maketrans(dict.fromkeys(list(string.punctuation)))\n",
    "    raw_text = raw_text.translate(trantab)  \n",
    "    # ----------- Tokenize ----------- #\n",
    "    tokens = nltk.word_tokenize(raw_text)\n",
    "    \n",
    "    # ----------- Case Folding ----------- #\n",
    "    # tokens = [token.lower() for token in tokens]\n",
    "    # ----------- Lemmatize ----------- #\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    # tokens = [lemmatizer.lemmatize(token, pos='v') for token in tokens]\n",
    "    # ----------- Stemming ----------- #\n",
    "    # stemmer = LancasterStemmer()\n",
    "    stemmer = PorterStemmer()\n",
    "    # tokens = [stemmer.stem(token) for token in tokens]\n",
    "    \n",
    "    # ----------- One shot approach ----------- #\n",
    "    # With Stemming\n",
    "    # tokens = [stemmer.stem(lemmatizer.lemmatize(token.lower(), pos='v')) for token in tokens]\n",
    "    # Without Stemming\n",
    "    tokens = [lemmatizer.lemmatize(token.lower(), pos='v') for token in tokens] # Could also detect word type (ADV, ADJ, VERB< NOUN) but this was too slow ...\n",
    "    \n",
    "    \n",
    "    return tokens\n",
    "\n",
    "prepare_text(\"Edvard was a Runner. He was always running. the adventures of sherlock holmes. students . classes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WVhVEO6VARIa"
   },
   "source": [
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=4>\n",
    "    <h1>\n",
    "    <b>شناسایی و حذف stop-words (۵ نمره)</b>\n",
    "    </h1>\n",
    "    این بخش باید توسط خودتان و بدون استفاده از کد آماده پیاده‌سازی شود. ترم‌های موجود در مجموعه دادگان را بر اساس تکرار آن‌ها مرتب کرده و پرتکرارترین آن‌ها را به عنوان stop-words در نظر بگیرید. اینکه چند ترم را به عنوان stop-words در نظر بگیرید به عهده خودتان است.<br>\n",
    "    با فراخوانی تابع ()get_stop_words لیست stop-words به همراه تعداد تکرار آن‌‌ها خروجی داده می‌شود.<br>\n",
    "    ترم‌های به دست آمده از این بخش نباید در نمایه حضور داشته باشند.\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "<b>گزارش : </b>\n",
    "با توجه به اینکه در صورت درج داکیومنت های جدید باید highlighting صورت بگیره (در سرچ) ، ما تمام داکیومنت های خام را در یک dataframe نگه می داریم که آن را آپدیت نگه داریم برای سرچ و هایلایت کردن\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>plot</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>The Suburbanite</td>\n",
       "      <td>The film is about a family who move to the sub...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Laughing Gas</td>\n",
       "      <td>The plot is that of a black woman going to the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>A Calamitous Elopement</td>\n",
       "      <td>A young couple decides to elope after being ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>The Call of the Wild</td>\n",
       "      <td>A white girl (Florence Lawrence) rejects a pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>A Lad from Old Ireland</td>\n",
       "      <td>An Irish boy (Olcott) emigrates to America to ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                   title  \\\n",
       "0   1         The Suburbanite   \n",
       "1   2            Laughing Gas   \n",
       "2   3  A Calamitous Elopement   \n",
       "3   4    The Call of the Wild   \n",
       "4   5  A Lad from Old Ireland   \n",
       "\n",
       "                                                plot  \n",
       "0  The film is about a family who move to the sub...  \n",
       "1  The plot is that of a black woman going to the...  \n",
       "2  A young couple decides to elope after being ca...  \n",
       "3  A white girl (Florence Lawrence) rejects a pro...  \n",
       "4  An Irish boy (Olcott) emigrates to America to ...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies = pd.read_csv(\"movies.csv\")\n",
    "movies.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "<b>گزارش : </b>\n",
    "برای این قسمت در ابتدا داکیومنت ها را از فایل میخوانیم و به کمک تابع prepare_text که در بالا زدیم ، عملیات tokenization رو انجام داده و سپس تعداد هر token را میشماریم و در انتها token های با بیشترین تعداد را خروجی میدهیم\n",
    "(* نکته : دقت کنید که این stop word ها از روی داکیومنت های حاضر در دیتافریم movies بدست می آید . بنابرین مقدار movies را بروز کنید در بلاک بالا)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B-0YxOUeBGDY",
    "outputId": "b8e3476a-02af-41dc-b4a3-5c637ffd4118"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 147374,\n",
       " 'to': 91561,\n",
       " 'and': 83077,\n",
       " 'be': 69029,\n",
       " 'a': 67762,\n",
       " 'of': 42764,\n",
       " 'in': 38565,\n",
       " 'his': 34704,\n",
       " 'he': 31677,\n",
       " 'her': 27923,\n",
       " 'that': 26232,\n",
       " 'with': 25049,\n",
       " 'have': 20186,\n",
       " 'him': 18898,\n",
       " 'by': 17817,\n",
       " 'for': 17580,\n",
       " 'she': 17221,\n",
       " 'as': 15615,\n",
       " 'on': 14842,\n",
       " 'but': 13503,\n",
       " 'they': 13158,\n",
       " 'who': 12620,\n",
       " 'at': 12334,\n",
       " 'from': 11095,\n",
       " 'an': 10855,\n",
       " 'when': 9988,\n",
       " 'their': 9475,\n",
       " 'after': 8875,\n",
       " 'it': 8725,\n",
       " 'out': 7833}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calc_stop_words(output_count):\n",
    "    countDict = {}\n",
    "    for index, row in movies.iterrows():\n",
    "        title, plot = row['title'], row['plot']\n",
    "        tokens = prepare_text(title) + prepare_text(plot)\n",
    "        for token in tokens:\n",
    "            countDict[token] = (countDict[token] if token in countDict else 0) + 1\n",
    "    \n",
    "    sorted_keys = sorted(countDict, key=countDict.get, reverse=True)\n",
    "    result = {key: countDict[key] for key in sorted_keys[:output_count]}\n",
    "    \n",
    "    return result\n",
    "\n",
    "def get_stop_words():\n",
    "    return calc_stop_words(30)\n",
    "\n",
    "stop_words = get_stop_words()\n",
    "stop_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A4Z02BzNHD1z"
   },
   "source": [
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=4>\n",
    "    <h1>\n",
    "    <b>نمایه‌سازی (۱۵ نمره)</b>\n",
    "    </h1>\n",
    "    در این بخش باید برای سامانه positional index بسازید. برای هر ترم باید مشخص باشد که آن ترم در تایتل چه فیلم‌هایی و در چه جایگاهی از تایتل هر فیلم قرار گرفته است. همچنین برای هر ترم باید مشخص باشد که آن ترم در طرح داستان چه فیلم‌هایی و در چه جایگاهی از طرح داستان هر فیلم قرار گرفته است.<br>\n",
    "     برای ارزیابی بهتر و عادلانه‌تر به ویژه برای ارزیابی بخش فشرده‌سازی، از استاندارد بیان شده در قطعه کد زیر برای ذخیره posting list هر ترم در RAM استفاده کنید.\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "<b>گزارش : </b>\n",
    "در این قسمت همانند توضیحات داده شده positional index مربوطه را از روی داکیومنت ها در فایل movies.csv میسازیم\n",
    "از این پس یک متغیر گلوبال به اسم positional_index خواهیم داشت     \n",
    " \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zJy4LiraBm8E",
    "outputId": "a740d94d-1896-4a6a-9a28-68b1cb8d32db"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56711"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_positional_index_for_token(positional_index, token):\n",
    "    if token not in positional_index:\n",
    "        positional_index[token] = [[], []]\n",
    "        \n",
    "def add_tokens_to_end_positional_index(positional_index, tokens, doc_id, is_title):\n",
    "    ind = 0 if is_title else 1\n",
    "    n = len(tokens)\n",
    "    \n",
    "    for i in range(n):\n",
    "        token = tokens[i]\n",
    "        \n",
    "        if token in stop_words:\n",
    "            continue\n",
    "        \n",
    "        init_positional_index_for_token(positional_index, token)\n",
    "        token_pos_index = positional_index[token][ind]\n",
    "        \n",
    "        if len(token_pos_index) == 0 or token_pos_index[-1][0] != doc_id:\n",
    "            token_pos_index.append([doc_id, [i+1]])\n",
    "        else:\n",
    "            token_pos_index[-1][1].append(i+1)            \n",
    "\n",
    "def create_positional_index():\n",
    "    global movies\n",
    "    movies = pd.read_csv(\"movies.csv\") # Just in case of some changes on the default (add new docs or ...)\n",
    "    \n",
    "    positional_index = {}\n",
    "    for index, row in movies.iterrows():\n",
    "        doc_id, title, plot = row['id'], row['title'], row['plot']\n",
    "        title_tokens, plot_tokens = prepare_text(title), prepare_text(plot)\n",
    "        add_tokens_to_end_positional_index(positional_index, title_tokens, doc_id, True)\n",
    "        add_tokens_to_end_positional_index(positional_index, plot_tokens, doc_id, False)\n",
    "    \n",
    "    return positional_index\n",
    "\n",
    "positional_index = create_positional_index()\n",
    "len(positional_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "voqMrk4rg1qk"
   },
   "source": [
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=4>\n",
    "    <h1>\n",
    "    <b>پویا‌سازی نمایه (۱۰ نمره)</b>\n",
    "    </h1>\n",
    "    نمایه ایجاد شده باید قابلیت حذف و اضافه تک داکیومنت را داشته باشد.\n",
    "    برای اضافه شدن داکیومنت، به تابع ()add_single_document یک رشته داده می‌شود که اطلاعات مربوط به داکیومنت شامل id و plot و title در آن با کاما جدا شده است. برای حذف داکیومنت نیز id آن به تابع ()remove_single_document داده می‌شود.<br>\n",
    "    تضمین می‌شود که شرط یکتا بودن id داکیومنت‌ها نقض نشود. برای مثال دو داکیومنت با شناسه یکسان به مجموعه اضافه نخواهد شد. البته ممکن است حذف شده و دوباره اضافه شود.\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "<b>گزارش : </b>\n",
    "در این قسمت امکان حذف یا درج یک داکیومنت در corpus فراهم میشود . در صورت انجام این عملیات ها علاوه بر آپدیت شدن positional_index ، مقدار دیتافریم movies نیز آپدیت می شود \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "metadata": {
    "id": "e1Ej2n3MB6ef"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[],\n",
       " [[466, [400]],\n",
       "  [721, [482]],\n",
       "  [854, [8]],\n",
       "  [1041, [38]],\n",
       "  [2124, [476]],\n",
       "  [3177, [518]],\n",
       "  [3920, [485]],\n",
       "  [4152, [297]],\n",
       "  [4273, [291]],\n",
       "  [4364, [222]],\n",
       "  [4667, [318]],\n",
       "  [4984, [368]],\n",
       "  [5020, [433]],\n",
       "  [5125, [608]],\n",
       "  [5131, [497]],\n",
       "  [5213, [159]],\n",
       "  [5319, [169]],\n",
       "  [5475, [166]],\n",
       "  [5539, [904]],\n",
       "  [5629, [259]],\n",
       "  [5654, [158]],\n",
       "  [5708, [31]],\n",
       "  [5855, [342]],\n",
       "  [5993, [540]],\n",
       "  [11111111, [2]],\n",
       "  [22222222, [8]]]]"
      ]
     },
     "execution_count": 615,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def add_new_document_tokens(doc_id, tokens, is_title):\n",
    "    ind = 0 if is_title else 1\n",
    "    n = len(tokens)\n",
    "    \n",
    "    for i in range(n):\n",
    "        token = tokens[i]\n",
    "        \n",
    "        if token in stop_words:\n",
    "            continue\n",
    "        \n",
    "        init_positional_index_for_token(positional_index, token)\n",
    "        token_pos_index = positional_index[token][ind]\n",
    "        \n",
    "        ind_to_insert = 0\n",
    "        while ind_to_insert < len(token_pos_index):\n",
    "            if doc_id > token_pos_index[ind_to_insert][0]:\n",
    "                ind_to_insert += 1\n",
    "            else:\n",
    "                break\n",
    "        \n",
    "        if ind_to_insert < len(token_pos_index) and token_pos_index[ind_to_insert][0] == doc_id:\n",
    "            token_pos_index[ind_to_insert][1].append(i+1)\n",
    "        else:\n",
    "            token_pos_index.insert(ind_to_insert, [doc_id, [i+1]])    \n",
    "\n",
    "def add_single_documnet(document):\n",
    "    global movies\n",
    "    \n",
    "    doc_id, title, plot = document.split(\",\", 2)\n",
    "    doc_id = int(doc_id)\n",
    "    \n",
    "    title_tokens, plot_tokens = prepare_text(title), prepare_text(plot)\n",
    "    \n",
    "    add_new_document_tokens(doc_id, title_tokens, True)\n",
    "    add_new_document_tokens(doc_id, plot_tokens, False)\n",
    "    \n",
    "    movies.loc[len(movies)] = [doc_id, title, plot]\n",
    "\n",
    "new_document = \"22222222,The Adventures of Sherlock Holmes,The picture begins with Moriarty and Holmes verbally sparring on the steps\"\n",
    "add_single_documnet(new_document)\n",
    "new_document = \"11111111,Adventures Holmes, steps verbally arman step\"\n",
    "add_single_documnet(new_document)\n",
    "positional_index['verbally']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 616,
   "metadata": {
    "id": "DsarzVzhDoMi"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[],\n",
       " [[466, [400]],\n",
       "  [721, [482]],\n",
       "  [854, [8]],\n",
       "  [1041, [38]],\n",
       "  [2124, [476]],\n",
       "  [3177, [518]],\n",
       "  [3920, [485]],\n",
       "  [4152, [297]],\n",
       "  [4273, [291]],\n",
       "  [4364, [222]],\n",
       "  [4667, [318]],\n",
       "  [4984, [368]],\n",
       "  [5020, [433]],\n",
       "  [5125, [608]],\n",
       "  [5131, [497]],\n",
       "  [5213, [159]],\n",
       "  [5319, [169]],\n",
       "  [5475, [166]],\n",
       "  [5539, [904]],\n",
       "  [5629, [259]],\n",
       "  [5654, [158]],\n",
       "  [5708, [31]],\n",
       "  [5855, [342]],\n",
       "  [5993, [540]],\n",
       "  [22222222, [8]]]]"
      ]
     },
     "execution_count": 616,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def binary_search(arr, l, h, x):\n",
    "    if h < l:\n",
    "        return -1\n",
    "    \n",
    "    mid = (l + h) // 2\n",
    "    if arr[mid][0] == x:\n",
    "        return mid\n",
    "    elif arr[mid][0] > x:\n",
    "        return binary_search(arr, l, mid - 1, x)\n",
    "    else:\n",
    "        return binary_search(arr, mid + 1, h, x)\n",
    "\n",
    "def remove_doc_from_list(arr, doc_id):\n",
    "    ind = binary_search(arr, 0, len(arr) - 1, doc_id)\n",
    "    if ind != -1:\n",
    "        arr.pop(ind)\n",
    "\n",
    "def remove_signle_document(document_id):\n",
    "    global movies\n",
    "    movies.drop(movies[movies['id']==1].index , inplace=True)\n",
    "    \n",
    "    empty_keys = []\n",
    "    for k in positional_index.keys():\n",
    "        remove_doc_from_list(positional_index[k][0], document_id)\n",
    "        remove_doc_from_list(positional_index[k][1], document_id)\n",
    "        if len(positional_index[k][0]) == 0 and len(positional_index[k][1]) == 0:\n",
    "            empty_keys.append(k)\n",
    "    \n",
    "    for k in empty_keys:\n",
    "        positional_index.pop(k)\n",
    "    \n",
    "    return\n",
    "  \n",
    "remove_signle_document(11111111)\n",
    "positional_index['verbally']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4PBbFcmpXFKD"
   },
   "source": [
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=4>\n",
    "    <h1>\n",
    "    <b>ذخیره و فشرده‌سازی نمایه (۲۰ نمره)</b>\n",
    "    </h1>\n",
    "    در این بخش باید توانایی ذخیره کردن نمایه و بارگذاری مجدد آن را به سامانه اضافه کنید. ذخیره‌سازی به ۳ روش صورت می‌گیرد. بدون فشرده‌سازی، فشرده‌سازی از روش gamma-code و فشرده‌سازی از روش variable-byte. روش‌های فشرده‌سازی باید توسط خودتان پیاده‌سازی شود.\n",
    "    برای ذخیره نمایه در فایل نیز از JSON  استفاده کنید.<br>\n",
    "     بخشی از نمره شما در این قسمت به میزان فشرده‌سازی نمایه اختصاص داده شده است. بنابراین پیاده‌سازی بهینه روش‌های فشرده‌سازی مهم است.\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "<b>گزارش : </b>\n",
    "    در این قسمت 2 کلاس برای 2 نحوه فشرده سازی (variable byte و gamma code)  پیاده سازی شده که در فایل های کناری قرار دارند که در قسمت زیر import شده اند\n",
    "    هم برای compress و هم برای decmopress در کلاس ها متد هایی تعریف شده که این کار را انجام می دهند . \n",
    "    . برای جایگزینی اعداد با gap size ها نیز تابع هایی به صورت generic نوشته شده اند که بسته به نوع compression تابع را روی آن ها اعمال می کنند.\n",
    ". همچنین صحت decompress کردن نیز با سایز داکیومنت کمتر تست شده که مقادیر یکسانی با قبل از فشرده سازی داشت\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "metadata": {},
   "outputs": [],
   "source": [
    "from variable_byte import VariableByteCompressor\n",
    "from gamma_code import GammaCodeCompressor\n",
    "\n",
    "variable_byte_compressor = VariableByteCompressor()\n",
    "gamma_code_compressor = GammaCodeCompressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 621,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zYN9I4_BD178",
    "outputId": "4a11351f-d214-495e-eb3f-7dc56863cd58"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Compression: 18273368\n",
      "Variable Byte Compression: 11980106 (34.44%)\n",
      "Gamma Code Compression: 12903570 (29.39%)\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------- Generic Methods ------------------------------------- #\n",
    "def replace_with_gap_size_and_compress(pos_index, compress_type):\n",
    "    if len(pos_index) == 0:\n",
    "        return []\n",
    "    \n",
    "    compress_list_func = variable_byte_compressor.compress_number_list if compress_type == 'variable_byte' else gamma_code_compressor.compress_number_list\n",
    "    \n",
    "    # -------------- Creates a compressed pos. index for a single doc -------------- #\n",
    "    def create_compressed_doc_pos_ind(doc_id, occur_list):\n",
    "        tmp_pos_index = [occur_list[0]]\n",
    "        n = len(occur_list)\n",
    "        prev = tmp_pos_index[0]\n",
    "        for i in range(1, n):\n",
    "            gap = occur_list[i] - prev\n",
    "            tmp_pos_index.append(gap)\n",
    "            prev = occur_list[i]\n",
    "        tmp_pos_index.insert(0, doc_id)\n",
    "        \n",
    "        return compress_list_func(tmp_pos_index)\n",
    "    \n",
    "    new_pos_index = []\n",
    "    new_pos_index.append(create_compressed_doc_pos_ind(pos_index[0][0], pos_index[0][1]))\n",
    "    \n",
    "    prev_doc = pos_index[0][0]\n",
    "    n = len(pos_index)\n",
    "    for doc in range(1, n):\n",
    "        doc_gap = pos_index[doc][0] - prev_doc\n",
    "        new_pos_index.append(create_compressed_doc_pos_ind(doc_gap, pos_index[doc][1]))\n",
    "        prev_doc = pos_index[doc][0]\n",
    "        \n",
    "    return new_pos_index\n",
    "# ----------------------------------------- Gamma Code ------------------------------------- #\n",
    "def gamma_code_compression_pos_index(pos_index):\n",
    "    return replace_with_gap_size_and_compress(pos_index, compress_type='gamma_code')\n",
    "\n",
    "def store_index_gamma_code(path):\n",
    "    new_pos_index = {}\n",
    "    \n",
    "    for word in positional_index:\n",
    "        new_pos_index[word] = [\n",
    "            gamma_code_compression_pos_index(positional_index[word][0]),\n",
    "            gamma_code_compression_pos_index(positional_index[word][1])\n",
    "        ]\n",
    "    \n",
    "    with open(path, \"w\") as outfile:  \n",
    "        json.dump(new_pos_index, outfile) \n",
    "# ----------------------------------------- Byte Variable ------------------------------------- #\n",
    "def variable_byte_compression_pos_index(pos_index):\n",
    "    return replace_with_gap_size_and_compress(pos_index, compress_type='variable_byte')\n",
    "\n",
    "def store_index_variable_byte(path):\n",
    "    new_pos_index = {}\n",
    "    \n",
    "    for word in positional_index:\n",
    "        new_pos_index[word] = [\n",
    "            variable_byte_compression_pos_index(positional_index[word][0]),\n",
    "            variable_byte_compression_pos_index(positional_index[word][1])\n",
    "        ]\n",
    "    \n",
    "    with open(path, \"w\") as outfile:  \n",
    "        json.dump(new_pos_index, outfile) \n",
    "# ----------------------------------------- No Compression ------------------------------------- #\n",
    "def store_index_no_compression(path):\n",
    "    with open(path, \"w\") as outfile:  \n",
    "        json.dump(positional_index, outfile) \n",
    "        return\n",
    "\n",
    "def store_index(path, compression_type):\n",
    "    function_map = {\n",
    "        \"no-compression\": store_index_no_compression,\n",
    "        \"variable-byte\": store_index_variable_byte,\n",
    "        \"gamma-code\": store_index_gamma_code,\n",
    "    }\n",
    "    function_map[compression_type](path)\n",
    "    size_of_file = os.path.getsize(path)\n",
    "    \n",
    "    return size_of_file\n",
    "\n",
    "no_compression_size = store_index(\"storage_no_compression.json\", \"no-compression\")\n",
    "print(\"No Compression:\", no_compression_size)\n",
    "variable_byte_size = store_index(\"storage_variable_byte.json\", \"variable-byte\")\n",
    "print(\"Variable Byte Compression:\", variable_byte_size, \"({:.2f}%)\".format((1-variable_byte_size/no_compression_size)*100))\n",
    "gamma_code_size = store_index(\"storage_gamma_code.json\", \"gamma-code\")\n",
    "print(\"Gamma Code Compression:\", gamma_code_size, \"({:.2f}%)\".format((1-gamma_code_size/no_compression_size)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 622,
   "metadata": {
    "id": "3Dm38ZXHFQVH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56711\n"
     ]
    }
   ],
   "source": [
    "def decompress_and_reverese_gap_size(pos_index, compress_type):\n",
    "    if len(pos_index) == 0:\n",
    "        return []\n",
    "    \n",
    "    decompress_func = variable_byte_compressor.decompress_string if compress_type == 'variable_byte' else gamma_code_compressor.decompress_string\n",
    "    \n",
    "    # -------------- Creates a compressed pos. index for a single doc -------------- #\n",
    "    def create_decompressed_doc_pos_ind(doc_pos_string, prev_doc):\n",
    "        all_numbers = decompress_func(doc_pos_string)\n",
    "        doc_id = all_numbers[0] + prev_doc\n",
    "        tmp_pos = [all_numbers[1]]\n",
    "        \n",
    "        for i in range(2, len(all_numbers)):\n",
    "            tmp_pos.append(all_numbers[i] + tmp_pos[-1])\n",
    "        \n",
    "        return [doc_id, tmp_pos]\n",
    "    \n",
    "    new_pos_index = []\n",
    "    new_pos_index.append(create_decompressed_doc_pos_ind(pos_index[0], 0))\n",
    "    \n",
    "    n = len(pos_index)\n",
    "    for doc in range(1, n):\n",
    "        new_pos_index.append(create_decompressed_doc_pos_ind(pos_index[doc], new_pos_index[doc-1][0]))\n",
    "        \n",
    "    return new_pos_index\n",
    "# ----------------------------------------- Gamma Code ------------------------------------- #\n",
    "def gamma_code_decompression_pos_index(pos_index):\n",
    "    return decompress_and_reverese_gap_size(pos_index, compress_type='gamma_code')\n",
    "\n",
    "def load_index_gamma_code(path):\n",
    "    global positional_index\n",
    "    \n",
    "    with open(path) as infile: \n",
    "        positional_index = json.load(infile)\n",
    "    \n",
    "    for word in positional_index:\n",
    "         positional_index[word] = [\n",
    "             gamma_code_decompression_pos_index(positional_index[word][0]),\n",
    "             gamma_code_decompression_pos_index(positional_index[word][1])\n",
    "         ]\n",
    "# ----------------------------------------- Variable Byte ------------------------------------- #\n",
    "def variable_byte_decompression_pos_index(pos_index):\n",
    "    return decompress_and_reverese_gap_size(pos_index, compress_type='variable_byte')\n",
    "\n",
    "def load_index_variable_byte(path):\n",
    "    global positional_index\n",
    "    \n",
    "    with open(path) as infile: \n",
    "        positional_index = json.load(infile)\n",
    "    \n",
    "    for word in positional_index:\n",
    "        positional_index[word] = [\n",
    "            variable_byte_decompression_pos_index(positional_index[word][0]),\n",
    "            variable_byte_decompression_pos_index(positional_index[word][1])\n",
    "        ]\n",
    "# ----------------------------------------- No Compression ------------------------------------- #\n",
    "def load_index_no_compression(path):\n",
    "    global positional_index\n",
    "    \n",
    "    with open(path) as infile: \n",
    "        positional_index = json.load(infile)\n",
    "        \n",
    "def load_index(path, compression_type):\n",
    "    function_map = {\n",
    "        \"no-compression\": load_index_no_compression,\n",
    "        \"variable-byte\": load_index_variable_byte,\n",
    "        \"gamma-code\": load_index_gamma_code\n",
    "    }\n",
    "    \n",
    "    function_map[compression_type](path)\n",
    "    \n",
    "    return\n",
    "\n",
    "# load_index(\"storage_no_compression.json\", \"no-compression\")\n",
    "# load_index(\"storage_variable_byte.json\", \"variable-byte\")\n",
    "# load_index(\"storage_gamma_code.json\", \"gamma-code\")\n",
    "print(len(positional_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4i-iLvu2nh2k"
   },
   "source": [
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=4>\n",
    "    <h1>\n",
    "    <b>اصلاح پرسمان (۱۵ نمره امتیازی)</b>\n",
    "    </h1>\n",
    "    در صورتی که پرسمان ورودی دارای غلط املایی باشد یا به عبارتی لغاتی از آن در لغت‌نامه موجود نباشد، لازم است که با جستجوی لغت‌های احتمالی و انتخاب بهترین لغت به ادامه‌ی جستجو با پرسمان اصلاح شده پرداخته شود. برای اینکار ابتدا باید با روش bigram و معیار jaccard نزدیک‌ترین لغات به لغت با غلط املایی را پیدا کنید. سپس با استفاده از معیار edit distance بهترین لغت را از میان آن‌ها بیابید.<br>\n",
    "    نیازی به ذخیره‌سازی و فشرده‌سازی نمایه بایگرم نیست. همچنین می‌توانید از کد آماده برای محاسبه edit distance استفاده کنید.\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "<b>گزارش : </b>\n",
    "    در این بخش ابتدا bigram را از روی داکیومنت ها می سازیم . سپس به کمک این bigram و استفاده از معیار jaccard یک مجموعه جواب محدود تر در میاوریم و سپس با استفاده از edit_distance بهترین جواب را در خروجی می دهیم\n",
    ". همچنین چند نمونه تست از توابع در زیر آمده است . \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "qMZTstsbL1G3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1621"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_bigram_index():\n",
    "    bigram = {}\n",
    "    \n",
    "    for index, row in movies.iterrows():\n",
    "        doc_id, title, plot = row['id'], row['title'], row['plot']\n",
    "        tokens = prepare_text(title) + prepare_text(plot)\n",
    "        for token in tokens:\n",
    "            token2 = \"$\" + token + \"$\"\n",
    "            for ind in range(len(token2)-1):\n",
    "                bichar = token2[ind:ind+2]\n",
    "                if bichar not in bigram:\n",
    "                    bigram[bichar] = set()\n",
    "                bigram[bichar].add(token)\n",
    "    \n",
    "    for k in bigram:\n",
    "        bigram[k] = list(bigram[k])\n",
    "    \n",
    "    return bigram\n",
    "\n",
    "bigram_global = create_bigram_index()\n",
    "len(bigram_global)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct\n",
      "correct\n",
      "correct\n"
     ]
    }
   ],
   "source": [
    "def jaccard_distance(lst1, lst2):\n",
    "    s1 = set([lst1[ind:ind+2] for ind in range(len(lst1)-1)])\n",
    "    s2 = set([lst2[ind:ind+2] for ind in range(len(lst2)-1)])\n",
    "    \n",
    "    return len(s1 & s2)/len(s1 | s2)\n",
    "\n",
    "def find_best_match(query):\n",
    "    query2 = \"$\" + query + \"$\"\n",
    "    candidates = set()\n",
    "    for ind in range(len(query2)-1):\n",
    "        bichar = query2[ind:ind+2]\n",
    "        for token in bigram_global[bichar]:\n",
    "            candidates.add(token)\n",
    "        \n",
    "    import math\n",
    "    \n",
    "    jaccard_limit = 0.4\n",
    "    best_candidate, best_distance = query, math.inf\n",
    "    for candidate in candidates:\n",
    "        if jaccard_distance(candidate, query) >= jaccard_limit:\n",
    "            tmp_distance = nltk.edit_distance(candidate, query, transpositions=False)\n",
    "            if tmp_distance < best_distance:\n",
    "                best_candidate, best_distance = candidate, tmp_distance\n",
    "    \n",
    "    \n",
    "    return best_candidate\n",
    "\n",
    "print(find_best_match(\"correct\"))\n",
    "print(find_best_match(\"crrect\"))\n",
    "print(find_best_match(\"corect\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "sL3E8C-nL1yG",
    "outputId": "d4da236e-728c-4419-e80e-e8a36bd4e85d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the dentures of hemlock holmes'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_corrected_text(raw_text):\n",
    "    tokens = prepare_text(raw_text)\n",
    "    suggestion = []\n",
    "\n",
    "    for token in tokens:\n",
    "        if token in positional_index or token in stop_words:\n",
    "            suggestion.append(token)\n",
    "            continue\n",
    "        \n",
    "        suggestion.append(find_best_match(token))\n",
    "    \n",
    "    corrected_text = \" \".join(suggestion)\n",
    "    \n",
    "    return corrected_text\n",
    "\n",
    "get_corrected_text(\"the adevntures of herlock holmes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-BWhdHJbs1zy"
   },
   "source": [
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=4>\n",
    "    <h1>\n",
    "    <b>جستجو و بازیابی اسناد (۲۵ نمره)</b>\n",
    "    </h1>\n",
    "    در این بخش جستجو ترتیب‌دار در فضای برداری tf-idf به روش <b>lnn-ltn</b> انجام می‌شود. یک پرسمان title و یک پرسمان plot ورودی گرفته شده و هر کدام در بخش مربوطه از اسناد جستجو می‌شوند. امتیاز نهایی هر سند برابر با جمع وزن‌دار امتیاز به دست آمده از جستجو در بخش title و plot است. به این صورت که وزن plot واحد در نظر گرفته شده و وزن تایتل به عنوان ورودی داده می‌شود.<br>\n",
    "    در نهایت اسناد برتر را نمایش دهید. تعداد حداکثر اسناد برتر نیز به عنوان ورودی داده می‌شود.<br><br>\n",
    "    نکته بسیار مهم نحوه نمایش اسناد انتخابی است. برای نمایش هر سند علاوه بر استفاده از شناسه و تیتر، یک هایلایت برای آن درست کنید. به این معنا که کلمات موجود در پرسمان را که باعث انتخاب سند شده‌اند به همراه ۲-۳ ترم قبل و بعد از آن به عنوان هایلایت آن سند نمایش دهید. اینگونه کاربر می‌تواند خیلی سریع دلیل بازیابی اسناد توسط سامانه را متوجه شود. مشابه کاری که سرچ گوگل انجام می‌دهد و ۲-۳ خط مربوطه را زیر وبسایت‌های پیشنهادی نمایش می‌دهد. طبیعتا راه حل بهینه برای این‌کار استفاده از قابلیت‌های نمایه جایگاهی می‌باشد.<br>\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "    <b>گزارش : </b>\n",
    "    در این قسمت برای پیاده سازی سرچ در ابتدا آن را اصلاح می کنیم (به کمک تابعی که در بالا تعریف کردیم)\n",
    "    سپس با استفاده از تابع search_single_query که کار آن محاسبه score و ... یک کویری برای یک بخش خاص می باشد ، score doc هارا بدست میاوریم و پس از merge کردن نتایج با استفاده از تابع highlight_text و prepare_output_result نتایج نهایی را آماده سازی میکنیم\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R0R2i_O4Gev_",
    "outputId": "7f57be29-eec0-489e-dd61-74d67c923106"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[4,\n",
       "  'the call of the <b>wild</b>',\n",
       "  '... white girl <b>florence</b> lawrence reject ... in film <b>florence</b> lawrence be ... the most <b>popular</b> among the ...'],\n",
       " [4864,\n",
       "  '<b>wild</b> things',\n",
       "  'a <b>popular</b> miami area ... wealthy and <b>popular</b> girl name ...'],\n",
       " [2238,\n",
       "  'davy crockett king of the <b>wild</b> frontier',\n",
       "  '... become a <b>popular</b> member of ...']]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def search_single_query(query, is_title):\n",
    "    from math import log10\n",
    "    \n",
    "    ind = 0 if is_title else 1\n",
    "    q_tokens = prepare_text(query)\n",
    "    \n",
    "    # --------- Query tf --------- #\n",
    "    q_tf_raw = {}\n",
    "    for t in q_tokens:\n",
    "        if t not in q_tf_raw:\n",
    "            q_tf_raw[t] = 0\n",
    "        q_tf_raw[t] += 1\n",
    "    q_tf_wt = {k: 1 + log10(v) for k, v in q_tf_raw.items()}\n",
    "    # --------- Query idf --------- #\n",
    "    N = len(movies)\n",
    "    df_t = {k: len(positional_index[k][ind]) if k in positional_index else 0 for k in q_tf_wt}\n",
    "    q_idf = {k: log10(N/v) if v != 0 else 0 for k, v in df_t.items()}\n",
    "    # --------- Query wt --------- #\n",
    "    q_wt = {k: q_tf_wt[k] * q_idf[k] for k in q_tf_wt.keys()}\n",
    "    \n",
    "    # --------- Term at a time approach --------- #\n",
    "    docs = {}\n",
    "    for t in q_wt.keys():\n",
    "        if t not in positional_index:\n",
    "            continue\n",
    "            \n",
    "        pos_list = positional_index[t][ind]\n",
    "        for d in pos_list:\n",
    "            doc_id = d[0]\n",
    "            doc_list = d[1]\n",
    "            \n",
    "            if doc_id not in docs:\n",
    "                docs[doc_id] = {\n",
    "                    \"score\": 0,\n",
    "                    \"occur\": [],\n",
    "                }\n",
    "            \n",
    "            d_tf = len(d[1])\n",
    "            d_wt = 1 + log10(d_tf)\n",
    "            docs[doc_id]['score'] += d_wt * q_wt[t]\n",
    "            docs[doc_id]['occur'] += d[1]\n",
    "    \n",
    "    return docs\n",
    "\n",
    "def highlight_text(text, keywords_pos, words_around=2):\n",
    "    if len(keywords_pos) == 0:\n",
    "        return \"\"\n",
    "    \n",
    "    # Dictionary for lookup in O(1)\n",
    "    keywords_pos_dict = {i for i in keywords_pos}\n",
    "    \n",
    "    # --------- Add words around the keywords --------- #\n",
    "    tokens = prepare_text(text)\n",
    "    show_ind = set()\n",
    "    n = len(tokens)\n",
    "    if words_around == -1: # this is for title case ...\n",
    "        show_ind = set([i for i in range(1, n+1)])\n",
    "    else :\n",
    "        for pos in keywords_pos:\n",
    "            for i in range(pos-words_around, pos+words_around+1):\n",
    "                show_ind.add(min(max(1, i), n))\n",
    "    # --------- Build highlighted result --------- #\n",
    "    show_ind = sorted(show_ind)\n",
    "    prev = show_ind[0]\n",
    "    result = [\"<b>\"+tokens[prev-1]+\"</b>\" if prev in keywords_pos_dict else tokens[prev-1]]\n",
    "    if prev > 1:\n",
    "        result.insert(0, '...')\n",
    "    for i in range(1, len(show_ind)):\n",
    "        ind = show_ind[i]\n",
    "        if ind != prev + 1:\n",
    "            result.append(\"...\")\n",
    "        token = tokens[ind-1]\n",
    "        if ind in keywords_pos_dict:\n",
    "            token = \"<b>\" + token + \"</b>\"\n",
    "        result.append(token)\n",
    "        prev = ind\n",
    "    if show_ind[-1] < n:\n",
    "        result.append('...')\n",
    "\n",
    "    return \" \".join(result)\n",
    "        \n",
    "\n",
    "def prepare_output_result(results):\n",
    "    output = []\n",
    "    for doc_id in results:\n",
    "        doc = movies[movies['id'] == doc_id]\n",
    "        doc_title, doc_plot = doc['title'].iat[0], doc['plot'].iat[0]\n",
    "        highlight = highlight_text(doc_plot, results[doc_id]['occur']['plot'])\n",
    "        output.append([\n",
    "            doc_id,\n",
    "            highlight_text(doc_title, results[doc_id]['occur']['title'], words_around=-1),\n",
    "            highlight.strip()\n",
    "        ])\n",
    "    \n",
    "    return output\n",
    "\n",
    "def search(title_query, plot_query, title_weight, max_result_count=10):\n",
    "    # ------------------------ Get corrected query text ------------------------ #\n",
    "    title_query, plot_query = get_corrected_text(title_query), get_corrected_text(plot_query)\n",
    "    # ------------------------ Search in title and plot and merge results ------------------------ #\n",
    "    title_search_result, plot_search_result = search_single_query(title_query, True), search_single_query(plot_query, False)\n",
    "    all_docs = set(list(title_search_result.keys()) + list(plot_search_result.keys()))\n",
    "    doc_results = {}\n",
    "    for doc in all_docs:\n",
    "        title_score = title_search_result[doc]['score'] if doc in title_search_result else 0\n",
    "        plot_score = plot_search_result[doc]['score'] if doc in plot_search_result else 0\n",
    "        doc_results[doc] = {}\n",
    "        doc_results[doc]['score'] = title_score*title_weight + plot_score\n",
    "        doc_results[doc]['occur'] = {\n",
    "            'title': title_search_result[doc]['occur'] if doc in title_search_result else [],\n",
    "            'plot': plot_search_result[doc]['occur'] if doc in plot_search_result else [],\n",
    "        }\n",
    "    result_size = min(max_result_count, len(doc_results))\n",
    "    top_k_results = {k: v for k, v in sorted(doc_results.items(), key=lambda item: item[1]['score'], reverse=True)[:result_size]}\n",
    "\n",
    "    return prepare_output_result(top_k_results)\n",
    "\n",
    "search(\"wild\", \"Florence popular\", 10, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gBig369C6wSC"
   },
   "source": [
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=4>\n",
    "    <h1>\n",
    "    <b>ارزیابی عملکرد سامانه (۱۰ نمره)</b>\n",
    "    </h1>\n",
    "    همانطور که در بخش مربوط به مجموعه دادگان گفته شد، تعدادی پرسمان نمونه به همراه اسناد مورد نظر برای آن‌ها در اختیار شما قرار گرفته است. در هر مورد اطلاعات لازم برای ایجاد یک پرسمان ذکر شده است. مطابق آن‌ها پرسمان خود را ایجاد کنید. نتایج به دست آمده از هر پرسمان را به عنوان predicted results آن پرسمان در نظر بگیرید. همچنین در هر مورد لیستی از شناسه‌ها وجود دارد. این لیست را به عنوان actual results در نظر بگیرید. <br>\n",
    "    معیار‌های زیر را پیاده‌سازی کنید (بدون استفاده از کد آماده) و نتیجه این معیارها را روی مجموعه‌ی actual و predicted گزارش کنید. دقت کنید که به ازای هر پرسمان باید تمام معیارها را در قالب یک جدول گزارش کنید.<br> دقت کنید که ۳ پرسمان آخر فقط برای افرادی که بخش spell correction را پیاده کرد‌ه‌اند نتیجه مطلوب دارد.\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------ Helper functions ------------ #\n",
    "def intersection(lst1, lst2):\n",
    "    return list(set(lst1) & set(lst2))\n",
    "\n",
    "def union(lst1, lst2):\n",
    "    return set(set(lst1) | set(lst2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aPfEVezQIsmb",
    "outputId": "28b08705-5d4f-47e6-baec-3372f3e3293c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3333333333333333"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def precision(actual, predicted):\n",
    "    return len(intersection(actual, predicted)) / len(predicted)\n",
    "\n",
    "precision([1,2], [1, 4, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "is4nfSw1LQoQ",
    "outputId": "cc2a4388-464b-4d67-cb86-32617e3e526d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def recall(actual, predicted):\n",
    "    return len(intersection(actual, predicted)) / len(actual)\n",
    "\n",
    "recall([1,2], [1, 4, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eYHYwRgTLVRA",
    "outputId": "d749daf0-102d-4855-e292-21dcd137d6be"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f1_score(actual, predicted):\n",
    "    p, r = precision(actual, predicted), recall(actual, predicted)\n",
    "    f1_score = (2*p*r)/(r + p) if r+p != 0 else 0\n",
    "    \n",
    "    return f1_score\n",
    "\n",
    "f1_score([1,2], [1, 4, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bZg-_PUPLZ31",
    "outputId": "e85b9efa-3b9b-470e-8b93-f474585da6c9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def map_score(actual, predicted):\n",
    "    sum_pa = 0\n",
    "    for i in range(len(predicted)):\n",
    "        d = predicted[i]\n",
    "        if d in actual:\n",
    "            sum_pa += precision(actual, predicted[:i+1])\n",
    "    map_score = sum_pa / len(intersection(actual, predicted)) if len(intersection(actual, predicted)) != 0 else 0\n",
    "    \n",
    "    return map_score\n",
    "\n",
    "map_score([1,2], [1, 4, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OgOKEbMNLf-r",
    "outputId": "b38f683a-7224-411b-efb4-8fe7ce4eb328"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ndcg_score(actual, predicted):\n",
    "    from math import log2\n",
    "    \n",
    "    dcg = int(predicted[0] in actual) +  sum([int(predicted[i] in actual)/log2(i+1) for i in range(1, len(predicted))])\n",
    "    max_dcg = 1 + sum([1/log2(i+1) for i in range(1, len(actual))])\n",
    "    ndcg_score = dcg/max_dcg\n",
    "    \n",
    "    return ndcg_score\n",
    "\n",
    "ndcg_score([1,2], [1, 4, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Validation #1 -------\n",
      "Predicted (size=10) : [626, 1812, 3297, 111, 74, 4213, 3296, 4507, 2760, 5312]\n",
      "Actual (size=10) : [626, 1812, 3297, 111, 74, 4213, 3296, 4507, 2760, 2039]\n",
      "Intersection (size=9) : [3296, 3297, 2760, 74, 111, 626, 1812, 4213, 4507]\n",
      "------- Validation #2 -------\n",
      "Predicted (size=2) : [5180, 3152]\n",
      "Actual (size=2) : [5180, 3152]\n",
      "Intersection (size=2) : [3152, 5180]\n",
      "------- Validation #3 -------\n",
      "Predicted (size=7) : [4643, 4471, 5122, 4836, 3661, 3517, 4105]\n",
      "Actual (size=7) : [4643, 4471, 5122, 4836, 3661, 3517, 4105]\n",
      "Intersection (size=7) : [5122, 4643, 4836, 4105, 3661, 4471, 3517]\n",
      "------- Validation #4 -------\n",
      "Predicted (size=12) : [4670, 4971, 5749, 5309, 5514, 4920, 1136, 4754, 1824, 722, 4160, 4631]\n",
      "Actual (size=12) : [4670, 4971, 5309, 1136, 4920, 5475, 2871, 4160, 4357, 794, 5749, 4754]\n",
      "Intersection (size=8) : [4160, 4971, 1136, 4754, 5749, 4920, 5309, 4670]\n",
      "------- Validation #5 -------\n",
      "Predicted (size=1) : [5903]\n",
      "Actual (size=1) : [5903]\n",
      "Intersection (size=1) : [5903]\n",
      "------- Validation #6 -------\n",
      "Predicted (size=1) : [5625]\n",
      "Actual (size=1) : [5625]\n",
      "Intersection (size=1) : [5625]\n",
      "------- Validation #7 -------\n",
      "Predicted (size=1) : [5153]\n",
      "Actual (size=1) : [5153]\n",
      "Intersection (size=1) : [5153]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>MAP Score</th>\n",
       "      <th>NDCG Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.942710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.911594</td>\n",
       "      <td>0.772243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Precision    Recall  F1 Score  MAP Score  NDCG Score\n",
       "0   0.900000  0.900000  0.900000   1.000000    0.942710\n",
       "1   1.000000  1.000000  1.000000   1.000000    1.000000\n",
       "2   1.000000  1.000000  1.000000   1.000000    1.000000\n",
       "3   0.666667  0.666667  0.666667   0.911594    0.772243\n",
       "4   1.000000  1.000000  1.000000   1.000000    1.000000\n",
       "5   1.000000  1.000000  1.000000   1.000000    1.000000\n",
       "6   1.000000  1.000000  1.000000   1.000000    1.000000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -------------- Validation -------------- #\n",
    "with open('validation.json') as f:\n",
    "    validations = json.load(f)\n",
    "    \n",
    "df = pd.DataFrame(columns=['Precision', 'Recall', 'F1 Score', 'MAP Score', 'NDCG Score'])\n",
    "v_cnt = 1\n",
    "for v in validations:\n",
    "    results = search(v['title_query'], v['plot_query'], v['title_weight'], v['max_size'])\n",
    "    predicted = [result[0] for result in results]\n",
    "    actual = v['doc_ids']\n",
    "    df.loc[len(df)] = [\n",
    "        precision(actual, predicted),\n",
    "        recall(actual, predicted),\n",
    "        f1_score(actual, predicted),\n",
    "        map_score(actual, predicted),\n",
    "        ndcg_score(actual, predicted)\n",
    "    ]\n",
    "    \n",
    "    # ----------------- Logging Validation result ----------------- #\n",
    "    print(\"------- Validation #{} -------\".format(v_cnt))\n",
    "    print(\"Predicted (size={}) :\".format(len(predicted)), predicted)\n",
    "    print(\"Actual (size={}) :\".format(len(actual)), actual)\n",
    "    print(\"Intersection (size={}) :\".format(len(intersection(predicted, actual))), intersection(predicted, actual))\n",
    "    v_cnt += 1\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SHVlw0kVBUW4"
   },
   "source": [
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=4>\n",
    "    <h1>\n",
    "    <b>نکات پایانی</b>\n",
    "    </h1>\n",
    "    \n",
    "\n",
    "1.   سیستم را بهینه پیاده‌سازی کنید تا در زمان کمتری بارگذاری و نمایه‌سازی  انجام شود.\n",
    "2.   تمام قطعه‌ کدهای بالا باید توسط شما تکمیل شود. نوت‌بوک نهایی باید بدون خطا اجرا شود. اگر تمام کدهای استفاده شده در همین فایل نیست،‌ حتما آن‌ها را نیز به همراه نوت‌بوک در کوئرا آپلود کنید.\n",
    "3.    اسم توابع و نحوه ورودی گرفتن و خروجی دادن آن‌ها را تغییر ندهید. بقیه اجزای توابع صرفا برای شهود بیشتر شما نوشته شده‌اند و نیازی به نگه‌داری آن‌ها نیست.\n",
    "4.   در صورت امکان استفاده از کد آماده مشخصا این مورد برای بخش مربوطه ذکر شده است. اگر چیزی در این مورد ذکر نشده نمی‌توانید از کد آماده استفاده کنید.\n",
    "5.    فایل داده‌ها را در کوئرا آپلود نکنید.\n",
    "6.    فایل زیپ نهایی و فایل نوت‌بوک حتما به فرمت StudentNumber_phase1 نامگذاری شود.\n",
    "\n",
    "\n",
    "<b>سالم و موفق باشید.</b>\n",
    "\n",
    "</font>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "MIR_Phase1_v1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
